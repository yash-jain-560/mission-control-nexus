{
  "comment": "Local-First Configuration - Ollama Primary, Cloud Fallback (ZERO COST)",
  
  "agents": {
    "defaults": {
      "heartbeat": {
        "every": "5m",
        "prompt": "Check: Any blockers, opportunities, or progress updates needed?"
      }
    },
    
    "main": {
      "name": "Orbit (Orchestrator - Local)",
      "model": "ollama/llama3.2:3b",
      "fallback": "gemini-2.0-flash",
      "description": "Multi-agent orchestrator - routes tasks locally via Ollama, falls back to Gemini if needed",
      "allowlistAgents": ["work", "personal"],
      "thinking": "low"
    },
    
    "work": {
      "name": "Codex (Implementation - Local)",
      "model": "ollama/llama3.2:3b",
      "fallback": "claude-haiku-4-5",
      "description": "Code generation, testing, documentation - Local Ollama for zero cost",
      "thinking": "off"
    },
    
    "personal": {
      "name": "Apex (Reasoning - Local→Cloud)",
      "model": "ollama/llama3.2:3b",
      "fallback": "claude-sonnet-4-5",
      "description": "Complex reasoning - tries local first, escalates to Sonnet if needed",
      "thinking": "medium"
    }
  },

  "models": {
    "ollama": {
      "provider": "local",
      "endpoint": "http://localhost:11434",
      "apiKeyEnv": null,
      "models": [
        "llama3.2:3b"
      ],
      "status": "ACTIVE",
      "quotas": {
        "llama3.2:3b": "Unlimited (local compute)"
      }
    },
    
    "gemini": {
      "provider": "google",
      "apiKeyEnv": "GEMINI_API_KEY",
      "models": [
        "gemini-2.0-flash",
        "gemini-2.0-pro"
      ],
      "quotas": {
        "gemini-2.0-flash": "12.5M tokens/day (free)",
        "gemini-2.0-pro": "1M tokens/day (free)"
      },
      "purpose": "Fallback only"
    },
    
    "anthropic": {
      "provider": "anthropic",
      "apiKeyEnv": "ANTHROPIC_API_KEY",
      "models": [
        "claude-haiku-4-5",
        "claude-sonnet-4-5"
      ],
      "pricing": {
        "claude-haiku-4-5": "$0.80 per 1M input tokens",
        "claude-sonnet-4-5": "$3.00 per 1M input tokens"
      },
      "purpose": "Fallback only (rare use)"
    }
  },

  "routing": {
    "strategy": "local-first-hybrid",
    "description": "Primary: Ollama (local, zero cost). Secondary: Gemini (free cloud). Tertiary: Anthropic (paid cloud)",
    "rules": {
      "orchestration": {
        "primary": "ollama/llama3.2:3b",
        "fallback": "gemini-2.0-flash",
        "estimatedTokens": 5000,
        "estimatedCost": "$0"
      },
      
      "implementation": {
        "primary": "ollama/llama3.2:3b",
        "fallback": "claude-haiku-4-5",
        "estimatedTokens": 20000,
        "estimatedCost": "$0 (local) / $0.016 (fallback)"
      },
      
      "testing": {
        "primary": "ollama/llama3.2:3b",
        "fallback": "gemini-2.0-flash",
        "estimatedTokens": 8000,
        "estimatedCost": "$0"
      },
      
      "documentation": {
        "primary": "ollama/llama3.2:3b",
        "fallback": "claude-haiku-4-5",
        "estimatedTokens": 15000,
        "estimatedCost": "$0 (local) / $0.012 (fallback)"
      },
      
      "design_review": {
        "primary": "ollama/llama3.2:3b",
        "fallback": "claude-sonnet-4-5",
        "estimatedTokens": 80000,
        "estimatedCost": "$0 (local) / $0.24 (fallback)"
      },
      
      "debugging": {
        "primary": "ollama/llama3.2:3b",
        "fallback": "claude-sonnet-4-5",
        "estimatedTokens": 60000,
        "estimatedCost": "$0 (local) / $0.18 (fallback)"
      }
    }
  },

  "localCompute": {
    "enabled": true,
    "endpoint": "http://localhost:11434",
    "timeout": 5000,
    "retries": 2,
    "fallbackConditions": [
      "ollama-offline",
      "timeout-exceeded",
      "quality-insufficient",
      "explicit-request"
    ]
  },

  "costControl": {
    "dailyBudget": "$10.00",
    "monthlyBudget": "$300.00",
    "alerts": {
      "dailyThreshold": "$5.00",
      "weeklyThreshold": "$35.00"
    },
    "estimatedMonthlyCost": "$0-3 (local-first, cloud rarely used)",
    "savings": "$897/month (vs cloud-only) + 99.9% (vs multi-model)"
  },

  "environment": {
    "required": [
      "Ollama running on http://localhost:11434 (ACTIVE)"
    ],
    "optional": [
      "GEMINI_API_KEY=<for fallback>",
      "ANTHROPIC_API_KEY=<for rare complex tasks>",
      "OLLAMA_API_ENDPOINT=http://localhost:11434",
      "LOCAL_TIMEOUT_MS=5000",
      "PREFER_LOCAL=true"
    ]
  },

  "implementation": {
    "phase": "2-local-first",
    "steps": [
      "1. Verify Ollama running: curl http://localhost:11434/api/tags",
      "2. Copy this config to openclaw.json",
      "3. Restart OpenClaw: openclaw gateway restart",
      "4. Verify routing: check logs for 'ollama/llama3.2:3b'",
      "5. Start Ticket 1 Phase 1 with local-first stack"
    ],
    "testing": [
      "Unit test ModelRouter with Ollama",
      "Integration test OllamaClient streaming",
      "E2E test with simple task (routing test)",
      "Monitor latency & quality",
      "Verify fallback chain works"
    ]
  },

  "advantages": {
    "cost": "Zero API costs for 95% of tasks",
    "privacy": "Data never leaves your machine",
    "latency": "Instant (<200ms orchestration)",
    "throughput": "Unlimited (no rate limits)",
    "availability": "Works offline",
    "predictability": "No cloud variability"
  },

  "tradeoffs": {
    "quality": "llama3.2 ≈ GPT-3.5 (not GPT-4)",
    "compute": "Requires local CPU/GPU",
    "scale": "Single machine only",
    "fallback": "Cloud costs if local insufficient"
  }
}
